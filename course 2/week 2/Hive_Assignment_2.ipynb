{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_select_new_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_select_new_table.hql\n",
    "USE stackoverflow_;\n",
    "--WITH plain_tags AS (\n",
    "--    SELECT year, tag, count(1) as popularity\n",
    "--    FROM posts \n",
    "--    LATERAL VIEW EXPLODE(tags) lateral_table as tag\n",
    "--    WHERE post_type_id==1\n",
    "--    GROUP BY year, tag\n",
    "--)\n",
    "--SELECT T2016.tag, rank_2016, rank_2009, popularity_2016, popularity_2009 FROM\n",
    "--    (\n",
    "--    SELECT tag, RANK() OVER(ORDER BY popularity DESC) as rank_2016, popularity as popularity_2016\n",
    "--    FROM plain_tags\n",
    "--    WHERE year==2016\n",
    "--    ) T2016\n",
    "--JOIN\n",
    "--    (\n",
    "--    SELECT tag, RANK() OVER(ORDER BY popularity DESC) as rank_2009, popularity as popularity_2009\n",
    "--    FROM plain_tags\n",
    "--    WHERE year==2009 \n",
    "--    ) T2009\n",
    "--ON T2016.tag==T2009.tag\n",
    "--ORDER BY rank_2016\n",
    "--LIMIT 10;\n",
    "\n",
    "SELECT T2016.tag, rank_2016, rank_2009, popularity_2016, popularity_2009 FROM\n",
    "    (\n",
    "    SELECT tag, RANK() OVER(ORDER BY popularity_2016 DESC) as rank_2016, popularity_2016\n",
    "    FROM \n",
    "        (\n",
    "        SELECT tag, count(1) as popularity_2016 \n",
    "        FROM posts\n",
    "        LATERAL VIEW EXPLODE(tags) lateral_table as tag\n",
    "        WHERE year==2016 AND post_type_id==1\n",
    "        GROUP BY tag, post_type_id\n",
    "        ) T1\n",
    "    ) T2016\n",
    "JOIN\n",
    "    (\n",
    "    SELECT tag, RANK() OVER(ORDER BY popularity_2009 DESC) as rank_2009, popularity_2009 \n",
    "    FROM \n",
    "        (\n",
    "        SELECT tag, count(1) as popularity_2009\n",
    "        FROM posts \n",
    "        LATERAL VIEW EXPLODE(tags) lateral_table as tag\n",
    "        WHERE year==2009 AND post_type_id==1\n",
    "        GROUP BY tag, post_type_id\n",
    "        ) T2 \n",
    "    ) T2009\n",
    "ON T2016.tag==T2009.tag\n",
    "ORDER BY rank_2016\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.144 seconds\n",
      "Query ID = jovyan_20190228154444_2608e905-abb9-49a6-8899-b19567749be3\n",
      "Total jobs = 8\n",
      "Launching Job 1 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1551360419868_0054, Tracking URL = http://2944b98cc2c8:8088/proxy/application_1551360419868_0054/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1551360419868_0054\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-02-28 15:44:57,883 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-02-28 15:45:09,555 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 8.15 sec\n",
      "2019-02-28 15:45:22,131 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.82 sec\n",
      "MapReduce Total cumulative CPU time: 13 seconds 820 msec\n",
      "Ended Job = job_1551360419868_0054\n",
      "Launching Job 2 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1551360419868_0055, Tracking URL = http://2944b98cc2c8:8088/proxy/application_1551360419868_0055/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1551360419868_0055\n",
      "Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1\n",
      "2019-02-28 15:45:41,899 Stage-5 map = 0%,  reduce = 0%\n",
      "2019-02-28 15:45:55,365 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 7.05 sec\n",
      "2019-02-28 15:46:06,582 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 12.49 sec\n",
      "MapReduce Total cumulative CPU time: 12 seconds 490 msec\n",
      "Ended Job = job_1551360419868_0055\n",
      "Launching Job 3 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1551360419868_0056, Tracking URL = http://2944b98cc2c8:8088/proxy/application_1551360419868_0056/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1551360419868_0056\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-02-28 15:46:27,537 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-02-28 15:46:37,821 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 4.14 sec\n",
      "2019-02-28 15:46:51,094 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 11.54 sec\n",
      "MapReduce Total cumulative CPU time: 11 seconds 540 msec\n",
      "Ended Job = job_1551360419868_0056\n",
      "Launching Job 4 out of 8\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1551360419868_0057, Tracking URL = http://2944b98cc2c8:8088/proxy/application_1551360419868_0057/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1551360419868_0057\n",
      "Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1\n",
      "2019-02-28 15:47:13,645 Stage-6 map = 0%,  reduce = 0%\n",
      "2019-02-28 15:47:26,572 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 2.74 sec\n",
      "2019-02-28 15:47:42,786 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 9.37 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 370 msec\n",
      "Ended Job = job_1551360419868_0057\n",
      "Stage-10 is selected by condition resolver.\n",
      "Stage-11 is filtered out by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Execution log at: /tmp/jovyan/jovyan_20190228154444_2608e905-abb9-49a6-8899-b19567749be3.log\n",
      "2019-02-28 03:47:49\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-02-28 03:47:51\tDump the side-table for tag: 1 with group count: 2369 into file: file:/tmp/jovyan/bf7e5869-b3b4-422f-8d71-374e53bd9b1f/hive_2019-02-28_15-44-33_851_7817041166147048083-1/-local-10008/HashTable-Stage-7/MapJoin-mapfile01--.hashtable\n",
      "2019-02-28 03:47:51\tUploaded 1 File to: file:/tmp/jovyan/bf7e5869-b3b4-422f-8d71-374e53bd9b1f/hive_2019-02-28_15-44-33_851_7817041166147048083-1/-local-10008/HashTable-Stage-7/MapJoin-mapfile01--.hashtable (76823 bytes)\n",
      "2019-02-28 03:47:51\tEnd of local task; Time Taken: 1.635 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 6 out of 8\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1551360419868_0058, Tracking URL = http://2944b98cc2c8:8088/proxy/application_1551360419868_0058/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1551360419868_0058\n",
      "Hadoop job information for Stage-7: number of mappers: 1; number of reducers: 0\n",
      "2019-02-28 15:48:03,392 Stage-7 map = 0%,  reduce = 0%\n",
      "2019-02-28 15:48:11,388 Stage-7 map = 100%,  reduce = 0%, Cumulative CPU 5.18 sec\n",
      "MapReduce Total cumulative CPU time: 5 seconds 180 msec\n",
      "Ended Job = job_1551360419868_0058\n",
      "Launching Job 7 out of 8\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1551360419868_0059, Tracking URL = http://2944b98cc2c8:8088/proxy/application_1551360419868_0059/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1551360419868_0059\n",
      "Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1\n",
      "2019-02-28 15:48:31,782 Stage-4 map = 0%,  reduce = 0%\n"
     ]
    }
   ],
   "source": [
    "! hive -f task1_select_new_table.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
